{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBDLlnmb9KRJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "mse = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sections in Encoder-Decoder architecture\n",
        "\n",
        "Encoder Transformer Block\n",
        "- Masked multi-head self attention\n",
        "- Add + Norm Layer\n",
        "- FCNN (for each output)\n",
        "- Add + Norm Layer\n",
        "\n",
        "Decoder Transformer Block\n",
        "- Multi-head cross attention\n",
        "- Add + Norm Layer\n",
        "- FCNN (for each output)\n",
        "- Add + Norm Layer\n",
        "\n",
        "\n",
        "Multi-head self-attention\n",
        "- Input $x_i$\n",
        "- Get query, value, and input $Qx_i$\n"
      ],
      "metadata": {
        "id": "sFd-acKo9SiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "# Self attention block\n",
        "dim = 1536\n",
        "\n",
        "\n",
        "\n",
        "class Multi_head_attention(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, vocab_size,  heads):\n",
        "    super(Multi_head_attention, self).__init__()\n",
        "    self.K = nn.Linear(dim, dim)\n",
        "    self.Q = nn.Linear(dim, dim)\n",
        "    self.V = nn.Linear(dim, dim)\n",
        "    self.ScaledDotProductAttention = None\n",
        "    self.heads = heads\n",
        "\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, input, mask):\n",
        "    V = self.split_tensor(self.V(input))\n",
        "    Q = self.split_tensor(self.Q(input))\n",
        "    K = self.split_tensor(self.K(input))\n",
        "    # Get attention\n",
        "    # ScaledDotProductAttention\n",
        "    out, attn  = self.ScaledDotProductAttention(Q, K, V)\n",
        "\n",
        "    out = self.concat(out)\n",
        "    # Get layers\n",
        "    # h = attn * V\n",
        "\n",
        "\n",
        "    # Now that we have the key, query, and value matrices, we can compute the values\n",
        "    #\n",
        "\n",
        "    # return self.\n",
        "\n",
        "  def split_tensor(self, input):\n",
        "\n",
        "    batch, length, d = input.size()\n",
        "    new_d = d / self.heads\n",
        "    return input.view(batch, length, self.head, new_d)\n",
        "\n",
        "\n",
        "    # Input: 3 dimensional\n",
        "    # First dimension- how many inputs\n",
        "    # Second dimension- length of each matrix\n",
        "    # So you can split up here\n",
        "    # Third dimension- dimensions\n",
        "\n",
        "\n",
        "\n",
        "#         # similar to group convolution (split by number of heads)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cn2XSS7G9Rjh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing\n",
        "- [ ] Positional encoding\n",
        "- [ ] Layernorm\n",
        "- [ ] Decoder Blocks\n",
        "- [ ] Encoder Blocks\n",
        "- [ ] Multi-head self/cross attention (with masking)\n",
        "- [ ] Tokenization"
      ],
      "metadata": {
        "id": "qADxlnmGwxQt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlrBlVBlwxHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8CRBQlF9-ca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}